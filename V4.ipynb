{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFbm2IHRyrfl5/c7OL9yN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SemihDemir13/chatbot-project-data/blob/main/V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4OtuaCGUdFt-"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "print(\"Gerekli kÃ¼tÃ¼phaneler kuruluyor...\")\n",
        "!pip install -q -U transformers huggingface_hub accelerate sentence-transformers torch langchain langchain-community langchain-huggingface chromadb pandas\n",
        "print(\"Kurulum tamamlandÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Token'Ä±nÄ± girdikten sonra 'Login successful' yazÄ±sÄ±nÄ± gÃ¶rmelisin.\n",
        "login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kpe-lcQXdR3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "print(\"Model yÃ¼kleniyor... (Biraz sabÄ±r)\")\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    # Colab GPU'su kullanÄ±lsÄ±n diye device_map=\"auto\" diyoruz\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Text generation pipeline'Ä± oluÅŸturuyoruz\n",
        "    llm_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=250 # CevabÄ±n uzunluÄŸu\n",
        "    )\n",
        "    print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: Model yÃ¼klenemedi. LÃ¼tfen HÃœCRE 2'de token girdiÄŸinizden ve modelin lisansÄ±nÄ± kabul ettiÄŸinizden emin olun.\\nHata: {e}\")\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "id": "pxrVTWwldi5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "print(\"CSV dosyasÄ± okunuyor ve veri formatlanÄ±yor...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('netflix_titles.csv')\n",
        "    df = df.fillna('Bilinmiyor')\n",
        "\n",
        "    dokumanlar = []\n",
        "\n",
        "    # TÃ¼m veriyi alÄ±yoruz\n",
        "    print(f\"Toplam {len(df)} adet kayÄ±t iÅŸleniyor...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # FormatÄ± sadeleÅŸtirdik. BaÅŸlÄ±k en Ã¼stte.\n",
        "        metin = (\n",
        "            f\"BaÅŸlÄ±k: {row['title']}\\n\"\n",
        "            f\"YÃ¶netmen: {row['director']}\\n\"\n",
        "            f\"YÄ±l: {row['release_year']}\\n\"\n",
        "            f\"TÃ¼r: {row['listed_in']}\\n\"\n",
        "            f\"Ã–zet: {row['description']}\\n\"\n",
        "            f\"Detay: Bu film {row['release_year']} yÄ±lÄ±nda yayÄ±nlanmÄ±ÅŸtÄ±r. {row['type']} kategorisindedir.\"\n",
        "        )\n",
        "        dokumanlar.append(metin)\n",
        "\n",
        "    print(f\"âœ… {len(dokumanlar)} adet iÃ§erik baÅŸarÄ±yla hazÄ±rlandÄ±.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: Dosya yok.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2KbD2AVjeQEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"VektÃ¶r veritabanÄ± (Ã‡ok Dilli) oluÅŸturuluyor...\")\n",
        "\n",
        "# DEÄžÄ°ÅžÄ°KLÄ°K BURADA:\n",
        "# ArtÄ±k TÃ¼rkÃ§e-Ä°ngilizce eÅŸleÅŸtirmesi yapabilen 'multilingual' modeli kullanÄ±yoruz.\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# ChromaDB veritabanÄ±nÄ± kur\n",
        "# EÄŸer hata alÄ±rsan Ã¶nceki veritabanÄ±nÄ± temizlemek gerekebilir ama genellikle Ã¼zerine yazar.\n",
        "vektor_veritabani = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "\n",
        "print(\"âœ… Ã‡ok dilli veritabanÄ± hazÄ±r! ArtÄ±k TÃ¼rkÃ§e sorgularÄ± Ä°ngilizce verilerle eÅŸleÅŸtirebilirim.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7LWL2D6zezqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def rag_yanit(soru, top_k=3):\n",
        "    # --- HAZIRLIK ---\n",
        "    etkisiz_kelimeler = [\"filmi\", \"film\", \"Ã¶ner\", \"bana\", \"hakkÄ±nda\", \"olan\", \"yapÄ±mÄ±\", \"yÃ¶netmen\", \"oyuncu\", \"kimdir\", \"listesi\", \"var\", \"mÄ±\", \"mu\", \"bir\", \"sÃ¶yle\", \"istiyorum\", \"tavsiye\", \"et\"]\n",
        "    temiz_soru = soru.lower()\n",
        "    for kelime in etkisiz_kelimeler:\n",
        "        temiz_soru = temiz_soru.replace(f\" {kelime} \", \" \").replace(f\" {kelime}\", \"\").replace(f\"{kelime} \", \"\")\n",
        "    anahtar_kelime = temiz_soru.strip()\n",
        "    anahtar_kelime_bitisik = anahtar_kelime.replace(\" \", \"\")\n",
        "\n",
        "    secilen_film_bilgisi = {} # Bulunan filmi burada tutacaÄŸÄ±z\n",
        "    kaynak_yontem = \"\"\n",
        "\n",
        "    # --- 1. ADIM: FÄ°LTRELEME (PANDAS) ---\n",
        "    yil_tespiti = re.search(r'(\\d{4})', soru)\n",
        "    uygun_filmler = pd.DataFrame()\n",
        "\n",
        "    # A) YIL VARSA\n",
        "    if yil_tespiti:\n",
        "        aranan_yil = int(yil_tespiti.group(1))\n",
        "        kaynak_yontem = f\"YIL FÄ°LTRESÄ° ({aranan_yil})\"\n",
        "        uygun_filmler = df[df['release_year'] == aranan_yil]\n",
        "\n",
        "    # B) Ä°SÄ°M VARSA\n",
        "    elif len(anahtar_kelime) > 2:\n",
        "        m_dir = df['director'].str.contains(anahtar_kelime, case=False, na=False) | df['director'].str.contains(anahtar_kelime_bitisik, case=False, na=False)\n",
        "        m_cast = df['cast'].str.contains(anahtar_kelime, case=False, na=False) | df['cast'].str.contains(anahtar_kelime_bitisik, case=False, na=False)\n",
        "        uygun_filmler = df[m_dir | m_cast]\n",
        "        if not uygun_filmler.empty:\n",
        "            kaynak_yontem = f\"Ä°SÄ°M FÄ°LTRESÄ° ('{anahtar_kelime}')\"\n",
        "\n",
        "    # --- 2. ADIM: VERÄ°YÄ° SEÃ‡ME ---\n",
        "    if not uygun_filmler.empty:\n",
        "        # Rastgele 1 film seÃ§\n",
        "        secilen = uygun_filmler.sample(n=1).iloc[0]\n",
        "        secilen_film_bilgisi = {\n",
        "            \"title\": secilen['title'],\n",
        "            \"year\": secilen['release_year'],\n",
        "            \"desc\": secilen['description']\n",
        "        }\n",
        "        print(f\"\\n[SÄ°STEM: {kaynak_yontem} bulundu: {secilen['title']}]\")\n",
        "\n",
        "    # C) HÄ°Ã‡BÄ°RÄ° YOKSA -> VEKTÃ–R ARAMASI\n",
        "    else:\n",
        "        kaynak_yontem = \"VEKTÃ–R ARAMASI\"\n",
        "        if len(anahtar_kelime) > 3 and not yil_tespiti:\n",
        "             print(f\"\\n[SÄ°STEM: '{anahtar_kelime}' bulunamadÄ±. Konuya gÃ¶re aranÄ±yor...]\")\n",
        "\n",
        "        # VektÃ¶rden en alakalÄ± 1 taneyi alÄ±yoruz (KafasÄ± karÄ±ÅŸmasÄ±n diye)\n",
        "        docs = vektor_veritabani.similarity_search(soru, k=1)\n",
        "        if docs:\n",
        "            doc = docs[0]\n",
        "            lines = doc.page_content.split('\\n')\n",
        "\n",
        "            # Metinden veriyi ayÄ±kla\n",
        "            title = \"Bilinmiyor\"\n",
        "            desc = \"AÃ§Ä±klama yok.\"\n",
        "            year = \"???\"\n",
        "\n",
        "            for line in lines:\n",
        "                if \"BaÅŸlÄ±k:\" in line: title = line.replace(\"BaÅŸlÄ±k: \", \"\").strip()\n",
        "                if \"Ã–zet:\" in line or \"Description:\" in line: desc = line.split(\":\", 1)[1].strip()\n",
        "                if \"YÄ±l:\" in line: year = line.replace(\"YÄ±l: \", \"\").strip()\n",
        "\n",
        "            secilen_film_bilgisi = {\"title\": title, \"year\": year, \"desc\": desc}\n",
        "            print(f\"\\n[SÄ°STEM: VektÃ¶rden bulundu: {title}]\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # --- 3. ADIM: SADECE Ã‡EVÄ°RÄ° PROMPTU ---\n",
        "    # EÄŸer hiÃ§ film bulunamadÄ±ysa\n",
        "    if not secilen_film_bilgisi:\n",
        "        return \"ÃœzgÃ¼nÃ¼m, kriterlerinize uygun bir film bulamadÄ±m.\"\n",
        "\n",
        "    ingilizce_ozet = secilen_film_bilgisi['desc']\n",
        "\n",
        "    # Modele Ã‡OK BASÄ°T bir gÃ¶rev veriyoruz: Sadece Ã§evir.\n",
        "    prompt = f\"\"\"Translate the following movie description from English to Turkish.\n",
        "\n",
        "English: {ingilizce_ozet}\n",
        "\n",
        "Turkish:\"\"\"\n",
        "\n",
        "    # Model Ã§eviriyi yapsÄ±n\n",
        "    try:\n",
        "        # max_new_tokens'Ä± kÄ±salttÄ±k ki saÃ§malamasÄ±n\n",
        "        sonuc = llm_pipeline(prompt, return_full_text=False, max_new_tokens=100)[0][\"generated_text\"]\n",
        "        turkce_ozet = sonuc.strip()\n",
        "        # EÄŸer model prompt'u tekrar ederse temizle\n",
        "        if \"Turkish:\" in turkce_ozet:\n",
        "            turkce_ozet = turkce_ozet.split(\"Turkish:\")[-1].strip()\n",
        "    except:\n",
        "        turkce_ozet = ingilizce_ozet # Hata olursa orijinalini bas\n",
        "\n",
        "    # --- 4. ADIM: CEVABI BÄ°Z BÄ°RLEÅžTÄ°RÄ°YORUZ ---\n",
        "    # FormatÄ± Python ile yapÄ±yoruz, bÃ¶ylece yapay zeka formatÄ± bozamaz.\n",
        "    final_cevap = f\"Ã–nerim: {secilen_film_bilgisi['title']} ({secilen_film_bilgisi['year']})\\nKonusu: {turkce_ozet}\"\n",
        "\n",
        "    return final_cevap"
      ],
      "metadata": {
        "id": "2WeDNuAGe3Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n================================================\")\n",
        "print(\"ðŸŽ¬ NETFLIX GURUSU (Ã‡Ä±kmak iÃ§in 'Ã§Ä±kÄ±ÅŸ' yazÄ±n)\")\n",
        "print(\"================================================\\n\")\n",
        "\n",
        "while True:\n",
        "    soru = input(\"Soru sor: \")\n",
        "    if soru.lower() in ['Ã§Ä±kÄ±ÅŸ', 'exit', 'quit']:\n",
        "        print(\"Ä°yi seyirler ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    print(\"ðŸ”Ž AraÅŸtÄ±rÄ±yorum...\")\n",
        "    cevap = rag_yanit(soru)\n",
        "    print(f\"\\nðŸ¤– AI: {cevap}\\n\")\n",
        "    print(\"-\" * 40)\n",
        ""
      ],
      "metadata": {
        "id": "2Xk8gvNbfExJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yv3Wy90ug1MI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}