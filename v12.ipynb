{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWCxyqaSAcWw2BZS4V80eX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SemihDemir13/chatbot-project-data/blob/main/v12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uqS9-Hiwwfol"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "print(\"Proje iÃ§in gerekli tÃ¼m kÃ¼tÃ¼phaneler kuruluyor...\")\n",
        "!pip install -q -U transformers huggingface_hub accelerate sentence-transformers torch langchain langchain-community langchain-huggingface chromadb pandas peft trl bitsandbytes datasets streamlit pyngrok\n",
        "print(\"âœ… Kurulum tamamlandÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Hugging Face hesabÄ±na giriÅŸ yap (Token soracak)\n",
        "login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9AoawqAYwmLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "# --- ADIM 1: VERÄ° HAZIRLIÄI ---\n",
        "print(\"ğŸ“Š Fine-Tuning iÃ§in mini veri seti hazÄ±rlanÄ±yor...\")\n",
        "try:\n",
        "    df_for_tuning = pd.read_csv('netflix_titles.csv').fillna('').head(100)\n",
        "\n",
        "    def format_for_tuning(row):\n",
        "        return f\"Movie: {row['title']}\\nType: {row['type']}\\nDescription: {row['description']}\"\n",
        "\n",
        "    dataset = Dataset.from_pandas(df_for_tuning)\n",
        "    dataset = dataset.map(lambda x: {\"text\": format_for_tuning(x)})\n",
        "    print(\"âœ… Mini eÄŸitim seti hazÄ±rlandÄ±.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: CSV dosyasÄ± okunurken sorun oldu. LÃ¼tfen dosyayÄ± yÃ¼kleyin. Detay: {e}\")\n",
        "\n",
        "# --- ADIM 2: MODELÄ° EÄÄ°TÄ°M MODUNDA YÃœKLEME ---\n",
        "print(\"\\nğŸ§  Model 4-bit (EÄŸitim Modu) olarak yÃ¼kleniyor...\")\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "# --- ADIM 3: FINE-TUNING ---\n",
        "print(\"\\nğŸ‹ï¸â€â™‚ï¸ Fine-Tuning BaÅŸlÄ±yor... (Bu iÅŸlem 2-3 dk sÃ¼rebilir)\")\n",
        "peft_config = LoraConfig(r=8, lora_alpha=16, task_type=\"CAUSAL_LM\")\n",
        "training_args = TrainingArguments(output_dir=\"./sonuclar\", per_device_train_batch_size=2, max_steps=30, logging_steps=10, learning_rate=2e-4)\n",
        "\n",
        "trainer = SFTTrainer(model=model, train_dataset=dataset, peft_config=peft_config, args=training_args)\n",
        "trainer.train()\n",
        "print(\"âœ… Fine-Tuning tamamlandÄ±!\")\n",
        "\n",
        "# --- ADIM 4: CHATBOT PÄ°PELÄ°NE'I OLUÅTURMA ---\n",
        "print(\"\\nğŸš€ Chatbot Pipeline oluÅŸturuluyor...\")\n",
        "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=250)\n",
        "print(\"âœ… Sistem kullanÄ±ma hazÄ±r!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AmZjRbykx7xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "print(\"RAG sistemi iÃ§in veri ve veritabanÄ± hazÄ±rlanÄ±yor...\")\n",
        "try:\n",
        "    df = pd.read_csv('netflix_titles.csv').fillna('Bilinmiyor')\n",
        "\n",
        "    # Konu bazlÄ± arama iÃ§in dokÃ¼manlarÄ± oluÅŸtur\n",
        "    dokumanlar = []\n",
        "    for index, row in df.sample(5000, random_state=42).iterrows(): # 5000 Ã¶rnek alalÄ±m\n",
        "        metin = f\"BaÅŸlÄ±k: {row['title']}\\nYÄ±l: {row['release_year']}\\nTÃ¼r: {row['listed_in']}\\nÃ–zet: {row['description']}\"\n",
        "        dokumanlar.append(metin)\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    vektor_veritabani = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "\n",
        "    print(f\"âœ… Ana DataFrame ({len(df)} satÄ±r) ve VektÃ¶r VeritabanÄ± ({len(dokumanlar)} dokÃ¼man) hazÄ±rlandÄ±.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: Veri hazÄ±rlanÄ±rken sorun oluÅŸtu. Detay: {e}\")"
      ],
      "metadata": {
        "id": "GOtX0K-8yKXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# --- SAYFA AYARLARI ---\n",
        "st.set_page_config(page_title=\"Netflix AI Chatbotu\", page_icon=\"ğŸ¬\", layout=\"centered\")\n",
        "st.title(\"ğŸ¬ Netflix AI Bot\")\n",
        "st.markdown(\"*Yapay Zeka Destekli Film & Dizi Ã–neri AsistanÄ±*\")\n",
        "\n",
        "# --- MODEL VE VERÄ° YÃœKLEME ---\n",
        "@st.cache_resource\n",
        "def sistemi_yukle():\n",
        "    print(\"Sistem web arayÃ¼zÃ¼ iÃ§in yÃ¼kleniyor...\")\n",
        "    df = pd.read_csv('netflix_titles.csv').fillna('Bilinmiyor')\n",
        "    model_id = \"Qwen/Qwen1.5-4B-Chat\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "    llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150)\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    dokumanlar = [f\"BaÅŸlÄ±k: {row['title']}\\nÃ–zet: {row['description']}\" for index, row in df.iterrows()]\n",
        "    vektor_db = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "    print(\"âœ… Web sistemi baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "    return df, llm_pipeline, vektor_db\n",
        "\n",
        "df, llm_pipeline, vektor_veritabani = sistemi_yukle()\n",
        "\n",
        "# --- RAG FONKSÄ°YONU (NÄ°HAÄ° VERSÄ°YON) ---\n",
        "def rag_yanit(soru):\n",
        "    def temizle_yorumlar(metin):\n",
        "        yasakli_ifadeler = [\"I hope this\", \"Please note\", \"In this translation\", \"What does\", \"The phrase means\", \"English:\", \"Turkish:\", \"Translation:\", \"å°\", \"Note:\", \"ë²ˆì—­ê²°ê³¼\", \"[\", \"]\"]\n",
        "        temiz_metin = metin\n",
        "        for ifade in yasakli_ifadeler:\n",
        "            if ifade in temiz_metin: temiz_metin = temiz_metin.split(ifade)[0]\n",
        "        temiz_metin = re.sub(r'[\\u4e00-\\u9fff]+', '', temiz_metin)\n",
        "        return temiz_metin.replace(\"ã€‚\", \"\").strip()\n",
        "\n",
        "    # ---- DÃœZELTME 1: YIL SINIRI KONTROLÃœ EKLENDÄ° ----\n",
        "    # Fonksiyonun en baÅŸÄ±nda, daha hiÃ§bir arama yapmadan, sorudaki yÄ±lÄ± kontrol et.\n",
        "    yil_tespiti_on_kontrol = re.search(r'(\\d{4})', soru)\n",
        "    if yil_tespiti_on_kontrol:\n",
        "        aranan_yil = int(yil_tespiti_on_kontrol.group(1))\n",
        "        min_yil = int(df['release_year'].min())\n",
        "        max_yil = int(df['release_year'].max())\n",
        "        if aranan_yil < min_yil or aranan_yil > max_yil:\n",
        "            return f\"ÃœzgÃ¼nÃ¼m, veri setimde sadece {min_yil} ile {max_yil} yÄ±llarÄ± arasÄ±ndaki iÃ§erikler bulunmaktadÄ±r.\"\n",
        "\n",
        "    tur_ceviri = {\"korku\": \"Horror\", \"gerilim\": \"Thriller\", \"aksiyon\": \"Action\", \"komedi\": \"Comed\", \"drama\": \"Dram\", \"bilim kurgu\": \"Sci-Fi\", \"romantik\": \"Romantic\", \"anime\": \"Anime\"}\n",
        "    etkisiz_kelimeler = [\"filmi\", \"film\", \"Ã¶ner\", \"bana\", \"yapÄ±mÄ±\", \"yÃ¶netmen\", \"oyuncu\", \"konusu\", \"nedir\", \"ne\", \"ile\", \"ilgili\", \"tavsiye\", \"et\"]\n",
        "    temiz_soru = soru.lower()\n",
        "    for kelime in etkisiz_kelimeler: temiz_soru = temiz_soru.replace(kelime, \"\").strip()\n",
        "    anahtar_kelimeler_listesi = temiz_soru.split()\n",
        "    secilen_film_bilgisi = {}; kaynak_yontem = \"\"; uygun_filmler = pd.DataFrame()\n",
        "\n",
        "    aranan_tur_eng = None\n",
        "    for tr_tur, eng_tur in tur_ceviri.items():\n",
        "        if tr_tur in soru.lower(): aranan_tur_eng = eng_tur; break\n",
        "\n",
        "    is_name_search = False\n",
        "\n",
        "    if yil_tespiti_on_kontrol: # YukarÄ±da zaten bulmuÅŸtuk, tekrar arama\n",
        "        aranan_yil = int(yil_tespiti_on_kontrol.group(1))\n",
        "        kaynak_yontem = f\"YIL ({aranan_yil})\"; uygun_filmler = df[df['release_year'] == aranan_yil]\n",
        "        if aranan_tur_eng and not uygun_filmler.empty:\n",
        "            tur_filtreli = uygun_filmler[uygun_filmler['listed_in'].str.contains(aranan_tur_eng, case=False)]\n",
        "            if not tur_filtreli.empty: uygun_filmler = tur_filtreli; kaynak_yontem += f\" + TÃœR ({aranan_tur_eng})\"\n",
        "    elif aranan_tur_eng:\n",
        "        kaynak_yontem = f\"TÃœR ({aranan_tur_eng})\"; uygun_filmler = df[df['listed_in'].str.contains(aranan_tur_eng, case=False)]\n",
        "    elif len(temiz_soru) > 2:\n",
        "        is_name_search = True\n",
        "        kaynak_yontem = f\"Ä°SÄ°M ARAMASI ('{temiz_soru}')\"\n",
        "        temp_df = df.copy()\n",
        "        for kelime in anahtar_kelimeler_listesi:\n",
        "            temp_df = temp_df[temp_df['cast'].str.contains(kelime, case=False) | temp_df['director'].str.contains(kelime, case=False)]\n",
        "        uygun_filmler = temp_df\n",
        "\n",
        "    if not uygun_filmler.empty:\n",
        "        secilen = uygun_filmler.sample(1).iloc[0]\n",
        "        secilen_film_bilgisi = {\"title\": secilen['title'], \"year\": secilen['release_year'], \"desc\": secilen['description'], \"genre\": secilen['listed_in']}\n",
        "    else:\n",
        "        if is_name_search:\n",
        "            return f\"ÃœzgÃ¼nÃ¼m, veritabanÄ±mda '{temiz_soru}' adÄ±nda bir yÃ¶netmen veya oyuncu bulamadÄ±m.\"\n",
        "        kaynak_yontem = \"VEKTÃ–R (KONU ARAMASI)\"\n",
        "        docs = vektor_veritabani.similarity_search(soru, k=1)\n",
        "        if docs:\n",
        "            bulunan_baslik = docs[0].page_content.split('\\n')[0].replace(\"BaÅŸlÄ±k: \", \"\")\n",
        "            tam_bilgi = df[df['title'] == bulunan_baslik]\n",
        "            if not tam_bilgi.empty:\n",
        "                secilen = tam_bilgi.iloc[0]\n",
        "                secilen_film_bilgisi = {\"title\": secilen['title'], \"year\": secilen['release_year'], \"desc\": secilen['description'], \"genre\": secilen['listed_in']}\n",
        "\n",
        "    if not secilen_film_bilgisi: return \"ÃœzgÃ¼nÃ¼m, uygun bir sonuÃ§ bulamadÄ±m.\"\n",
        "\n",
        "    # ---- DÃœZELTME 2: DAHA DA GÃœÃ‡LENDÄ°RÄ°LMÄ°Å PROMPT ----\n",
        "    ingilizce_ozet = secilen_film_bilgisi['desc']\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are a professional translator. Your ONLY task is to translate the user's English text into natural and fluent Turkish.\n",
        "RULES:\n",
        "1. ONLY provide the Turkish translation.\n",
        "2. DO NOT add any extra notes, comments, or characters from other languages.\n",
        "3. If the English text is \"Bilinmiyor\", translate it as \"Konu bilgisi bulunmuyor.\".\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{ingilizce_ozet}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        sonuc = llm_pipeline(prompt, return_full_text=False, max_new_tokens=150)[0][\"generated_text\"]\n",
        "        turkce_ozet = temizle_yorumlar(sonuc)\n",
        "        if len(turkce_ozet) < 5: turkce_ozet = \"Konu bilgisi alÄ±namadÄ±.\"\n",
        "    except: turkce_ozet = \"Konu Ã§evrilirken bir hata oluÅŸtu.\"\n",
        "\n",
        "    return (f\"**Ã–nerim:** {secilen_film_bilgisi['title']} ({secilen_film_bilgisi['year']})\\n\"\n",
        "            f\"**TÃ¼r:** {secilen_film_bilgisi['genre']}\\n\"\n",
        "            f\"**Konusu:** {turkce_ozet}\")\n",
        "\n",
        "# --- CHAT ARAYÃœZÃœ ---\n",
        "if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]): st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"2019 yapÄ±mÄ± korku filmi Ã¶ner...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"): st.markdown(prompt)\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"ArÅŸivde aranÄ±yor...\"):\n",
        "            response = rag_yanit(prompt)\n",
        "            st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "BYAVld-AzLVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Ã–NEMLÄ°: Sol menÃ¼deki Anahtar (ğŸ”‘) ikonuna tÄ±klayÄ±p\n",
        "# 'NGROK_AUTH_TOKEN' adÄ±nda bir sÄ±r oluÅŸturduÄŸundan ve token'Ä±nÄ± oraya yapÄ±ÅŸtÄ±rdÄ±ÄŸÄ±ndan emin ol.\n",
        "try:\n",
        "    authtoken = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    ngrok.set_auth_token(authtoken)\n",
        "    public_url = ngrok.connect(8501)\n",
        "\n",
        "    print(f\"âœ… Web Siten HazÄ±r! Linkin bu: {public_url}\")\n",
        "    print(\"Not: Site aÃ§Ä±ldÄ±ktan sonra modelin yÃ¼klenmesi 3-5 dakika sÃ¼rebilir.\")\n",
        "\n",
        "    # Arka planda Streamlit'i baÅŸlat\n",
        "    !streamlit run app.py &\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: ngrok token'Ä±nÄ± Colab Secrets'e eklediÄŸinden emin misin?\")\n",
        "    print(f\"LÃ¼tfen sol menÃ¼deki Anahtar (ğŸ”‘) ikonuna tÄ±kla ve 'NGROK_AUTH_TOKEN' adÄ±nda bir sÄ±r oluÅŸtur.\")"
      ],
      "metadata": {
        "id": "tXiNPWl2zcRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6235620-b66e-48a5-9491-1d92988693a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6PDGUzpzeWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}