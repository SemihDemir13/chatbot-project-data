{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhpjrFQThkTf0cYvc/xyGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SemihDemir13/chatbot-project-data/blob/main/V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4OtuaCGUdFt-"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "print(\"Gerekli kÃ¼tÃ¼phaneler kuruluyor...\")\n",
        "!pip install -q -U transformers huggingface_hub accelerate sentence-transformers torch langchain langchain-community langchain-huggingface chromadb pandas\n",
        "print(\"Kurulum tamamlandÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Token'Ä±nÄ± girdikten sonra 'Login successful' yazÄ±sÄ±nÄ± gÃ¶rmelisin.\n",
        "login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kpe-lcQXdR3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "print(\"Model yÃ¼kleniyor... (Biraz sabÄ±r)\")\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    # Colab GPU'su kullanÄ±lsÄ±n diye device_map=\"auto\" diyoruz\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Text generation pipeline'Ä± oluÅŸturuyoruz\n",
        "    llm_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=250 # CevabÄ±n uzunluÄŸu\n",
        "    )\n",
        "    print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: Model yÃ¼klenemedi. LÃ¼tfen HÃœCRE 2'de token girdiÄŸinizden ve modelin lisansÄ±nÄ± kabul ettiÄŸinizden emin olun.\\nHata: {e}\")\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "id": "pxrVTWwldi5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "print(\"CSV dosyasÄ± okunuyor ve veri formatlanÄ±yor...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('netflix_titles.csv')\n",
        "    df = df.fillna('Bilinmiyor')\n",
        "\n",
        "    dokumanlar = []\n",
        "\n",
        "    # TÃ¼m veriyi alÄ±yoruz\n",
        "    print(f\"Toplam {len(df)} adet kayÄ±t iÅŸleniyor...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # FormatÄ± sadeleÅŸtirdik. BaÅŸlÄ±k en Ã¼stte.\n",
        "        metin = (\n",
        "            f\"BaÅŸlÄ±k: {row['title']}\\n\"\n",
        "            f\"YÃ¶netmen: {row['director']}\\n\"\n",
        "            f\"YÄ±l: {row['release_year']}\\n\"\n",
        "            f\"TÃ¼r: {row['listed_in']}\\n\"\n",
        "            f\"Ã–zet: {row['description']}\\n\"\n",
        "            f\"Detay: Bu film {row['release_year']} yÄ±lÄ±nda yayÄ±nlanmÄ±ÅŸtÄ±r. {row['type']} kategorisindedir.\"\n",
        "        )\n",
        "        dokumanlar.append(metin)\n",
        "\n",
        "    print(f\"âœ… {len(dokumanlar)} adet iÃ§erik baÅŸarÄ±yla hazÄ±rlandÄ±.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: Dosya yok.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2KbD2AVjeQEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"VektÃ¶r veritabanÄ± (Ã‡ok Dilli) oluÅŸturuluyor...\")\n",
        "\n",
        "# DEÄÄ°ÅÄ°KLÄ°K BURADA:\n",
        "# ArtÄ±k TÃ¼rkÃ§e-Ä°ngilizce eÅŸleÅŸtirmesi yapabilen 'multilingual' modeli kullanÄ±yoruz.\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# ChromaDB veritabanÄ±nÄ± kur\n",
        "# EÄŸer hata alÄ±rsan Ã¶nceki veritabanÄ±nÄ± temizlemek gerekebilir ama genellikle Ã¼zerine yazar.\n",
        "vektor_veritabani = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "\n",
        "print(\"âœ… Ã‡ok dilli veritabanÄ± hazÄ±r! ArtÄ±k TÃ¼rkÃ§e sorgularÄ± Ä°ngilizce verilerle eÅŸleÅŸtirebilirim.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7LWL2D6zezqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def temizle_yorumlar(metin):\n",
        "    \"\"\"Modelin sonuna eklediÄŸi Ä°ngilizce yorumlarÄ± ve bozuk kÄ±sÄ±mlarÄ± siler.\"\"\"\n",
        "    yasakli_cumleler = [\n",
        "        \"I hope this\", \"Please note\", \"This phrase\", \"The phrase means\",\n",
        "        \"This is a\", \"Translated\", \"Here is\", \"What does\", \"English:\", \"Turkish:\"\n",
        "    ]\n",
        "    satirlar = metin.split('\\n')\n",
        "    temiz_satirlar = []\n",
        "    for satir in satirlar:\n",
        "        if any(yasak in satir for yasak in yasakli_cumleler):\n",
        "            break\n",
        "        if satir.strip():\n",
        "            temiz_satirlar.append(satir)\n",
        "    return \" \".join(temiz_satirlar).strip()\n",
        "\n",
        "def rag_yanit(soru, top_k=3):\n",
        "    # --- HAZIRLIK ---\n",
        "    tur_ceviri = {\n",
        "        \"korku\": \"Horror\", \"gerilim\": \"Thriller\", \"aksiyon\": \"Action\",\n",
        "        \"komedi\": \"Comed\", \"drama\": \"Dram\", \"romantik\": \"Romantic\", \"aÅŸk\": \"Romantic\",\n",
        "        \"bilim kurgu\": \"Sci-Fi\", \"belgesel\": \"Documentary\", \"Ã§ocuk\": \"Children\",\n",
        "        \"aile\": \"Family\", \"anime\": \"Anime\", \"macera\": \"Adventure\", \"suÃ§\": \"Crime\"\n",
        "    }\n",
        "\n",
        "    etkisiz_kelimeler = [\"filmi\", \"film\", \"Ã¶ner\", \"bana\", \"hakkÄ±nda\", \"olan\", \"yapÄ±mÄ±\", \"yÃ¶netmen\", \"oyuncu\", \"kimdir\", \"listesi\", \"var\", \"mÄ±\", \"mu\", \"bir\", \"sÃ¶yle\", \"istiyorum\", \"tavsiye\", \"et\", \"izle\"]\n",
        "\n",
        "    temiz_soru = soru.lower()\n",
        "    for kelime in etkisiz_kelimeler:\n",
        "        temiz_soru = temiz_soru.replace(f\" {kelime} \", \" \").replace(f\" {kelime}\", \"\").replace(f\"{kelime} \", \"\")\n",
        "\n",
        "    anahtar_kelime = temiz_soru.strip()\n",
        "    anahtar_kelimeler_listesi = anahtar_kelime.split()\n",
        "\n",
        "    secilen_film_bilgisi = {}\n",
        "    kaynak_yontem = \"\"\n",
        "    uygun_filmler = pd.DataFrame()\n",
        "\n",
        "    # --- 1. ADIM: ANALÄ°Z ---\n",
        "    yil_tespiti = re.search(r'(\\d{4})', soru)\n",
        "    min_yil = int(df['release_year'].min())\n",
        "    max_yil = int(df['release_year'].max())\n",
        "\n",
        "    # TÃ¼r Tespiti\n",
        "    aranan_tur_eng = None\n",
        "    for tr_tur, eng_tur in tur_ceviri.items():\n",
        "        if tr_tur in soru.lower():\n",
        "            aranan_tur_eng = eng_tur\n",
        "            break\n",
        "\n",
        "    # --- 2. ADIM: ARAMA SENARYOLARI ---\n",
        "\n",
        "    # SENARYO A: YIL VARSA\n",
        "    if yil_tespiti:\n",
        "        aranan_yil = int(yil_tespiti.group(1))\n",
        "        if aranan_yil < min_yil or aranan_yil > max_yil:\n",
        "            return f\"ÃœzgÃ¼nÃ¼m, veri setimde sadece {min_yil} ile {max_yil} yÄ±llarÄ± arasÄ±ndaki filmler bulunmaktadÄ±r. ({aranan_yil} mevcut deÄŸil)\"\n",
        "\n",
        "        kaynak_yontem = f\"YIL FÄ°LTRESÄ° ({aranan_yil})\"\n",
        "        uygun_filmler = df[df['release_year'] == aranan_yil]\n",
        "\n",
        "        # YÄ±lÄ±n iÃ§inde TÃ¼r de varsa sÃ¼z\n",
        "        if aranan_tur_eng and not uygun_filmler.empty:\n",
        "            tur_filtreli = uygun_filmler[uygun_filmler['listed_in'].str.contains(aranan_tur_eng, case=False, na=False)]\n",
        "            if not tur_filtreli.empty:\n",
        "                uygun_filmler = tur_filtreli\n",
        "                kaynak_yontem += f\" + TÃœR ({aranan_tur_eng})\"\n",
        "\n",
        "    # SENARYO B: SADECE TÃœR VARSA (YENÄ° EKLENDÄ°!)\n",
        "    # EÄŸer yÄ±l yoksa ama kullanÄ±cÄ± \"Korku filmi\" dediyse buraya girecek.\n",
        "    elif aranan_tur_eng:\n",
        "        kaynak_yontem = f\"TÃœR FÄ°LTRESÄ° ({aranan_tur_eng})\"\n",
        "        uygun_filmler = df[df['listed_in'].str.contains(aranan_tur_eng, case=False, na=False)]\n",
        "\n",
        "    # SENARYO C: Ä°SÄ°M VARSA (YÄ±l ve TÃ¼r yoksa isimdir)\n",
        "    elif len(anahtar_kelime) > 2:\n",
        "        temp_df_cast = df.copy()\n",
        "        temp_df_dir = df.copy()\n",
        "        for kelime in anahtar_kelimeler_listesi:\n",
        "            temp_df_cast = temp_df_cast[temp_df_cast['cast'].str.contains(kelime, case=False, na=False)]\n",
        "            temp_df_dir = temp_df_dir[temp_df_dir['director'].str.contains(kelime, case=False, na=False)]\n",
        "\n",
        "        uygun_filmler = pd.concat([temp_df_cast, temp_df_dir]).drop_duplicates()\n",
        "        if not uygun_filmler.empty:\n",
        "            kaynak_yontem = f\"Ä°SÄ°M FÄ°LTRESÄ° ('{anahtar_kelime}')\"\n",
        "\n",
        "    # --- 3. ADIM: VERÄ°YÄ° SEÃ‡ME ---\n",
        "    if not uygun_filmler.empty:\n",
        "        secilen = uygun_filmler.sample(n=1).iloc[0]\n",
        "        secilen_film_bilgisi = {\n",
        "            \"title\": secilen['title'],\n",
        "            \"year\": secilen['release_year'],\n",
        "            \"desc\": secilen['description'],\n",
        "            \"genre\": secilen['listed_in']\n",
        "        }\n",
        "        print(f\"\\n[SÄ°STEM: {kaynak_yontem} bulundu: {secilen['title']}]\")\n",
        "\n",
        "    # SENARYO D: VEKTÃ–R ARAMASI (Son Ã‡are)\n",
        "    else:\n",
        "        if yil_tespiti: return f\"AradÄ±ÄŸÄ±nÄ±z kriterlere uygun film bulunamadÄ±.\"\n",
        "\n",
        "        kaynak_yontem = \"VEKTÃ–R ARAMASI\"\n",
        "        docs = vektor_veritabani.similarity_search(soru, k=1)\n",
        "        if docs:\n",
        "            doc = docs[0]\n",
        "            lines = doc.page_content.split('\\n')\n",
        "            title, desc, year, genre = \"Bilinmiyor\", \"AÃ§Ä±klama yok.\", \"???\", \"???\"\n",
        "            for line in lines:\n",
        "                if \"BaÅŸlÄ±k:\" in line: title = line.replace(\"BaÅŸlÄ±k: \", \"\").strip()\n",
        "                if \"Ã–zet:\" in line or \"Description:\" in line: desc = line.split(\":\", 1)[1].strip()\n",
        "                if \"YÄ±l:\" in line: year = line.replace(\"YÄ±l: \", \"\").strip()\n",
        "                if \"TÃ¼r:\" in line: genre = line.replace(\"TÃ¼r: \", \"\").strip()\n",
        "\n",
        "            secilen_film_bilgisi = {\"title\": title, \"year\": year, \"desc\": desc, \"genre\": genre}\n",
        "            print(f\"\\n[SÄ°STEM: VektÃ¶rden bulundu: {title}]\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # --- 4. ADIM: Ã‡EVÄ°RÄ° ---\n",
        "    if not secilen_film_bilgisi: return \"ÃœzgÃ¼nÃ¼m, kriterlerinize uygun bir film bulamadÄ±m.\"\n",
        "\n",
        "    ingilizce_ozet = secilen_film_bilgisi['desc']\n",
        "    prompt = f\"Task: Translate the text below into fluent Turkish. Do not explain. Text: {ingilizce_ozet}\\nTranslation:\"\n",
        "\n",
        "    try:\n",
        "        sonuc = llm_pipeline(prompt, return_full_text=False, max_new_tokens=100)[0][\"generated_text\"]\n",
        "        turkce_ozet = temizle_yorumlar(sonuc.strip())\n",
        "        if len(turkce_ozet) < 5: turkce_ozet = ingilizce_ozet\n",
        "    except:\n",
        "        turkce_ozet = ingilizce_ozet\n",
        "\n",
        "    # --- 5. ADIM: CEVAP ---\n",
        "    final_cevap = (f\"Ã–nerim: {secilen_film_bilgisi['title']} ({secilen_film_bilgisi['year']})\\n\"\n",
        "                   f\"TÃ¼r: {secilen_film_bilgisi['genre']}\\n\"\n",
        "                   f\"Konusu: {turkce_ozet}\")\n",
        "\n",
        "    return final_cevap"
      ],
      "metadata": {
        "id": "2WeDNuAGe3Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n================================================\")\n",
        "print(\"ğŸ¬ NETFLIX GURUSU (Ã‡Ä±kmak iÃ§in 'Ã§Ä±kÄ±ÅŸ' yazÄ±n)\")\n",
        "print(\"================================================\\n\")\n",
        "\n",
        "while True:\n",
        "    soru = input(\"Soru sor: \")\n",
        "    if soru.lower() in ['Ã§Ä±kÄ±ÅŸ', 'exit', 'quit']:\n",
        "        print(\"Ä°yi seyirler ğŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    print(\"ğŸ” AraÅŸtÄ±rÄ±yorum...\")\n",
        "    cevap = rag_yanit(soru)\n",
        "    print(f\"\\nğŸ¤– AI: {cevap}\\n\")\n",
        "    print(\"-\" * 40)\n",
        ""
      ],
      "metadata": {
        "id": "2Xk8gvNbfExJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OXhvDYwIu8kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yv3Wy90ug1MI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}