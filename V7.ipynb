{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP89co8jlh501vJn4Ptdzwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SemihDemir13/chatbot-project-data/blob/main/V7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NFR43TqujP6o"
      },
      "outputs": [],
      "source": [
        "print(\"TÃ¼m kÃ¼tÃ¼phaneler (EÄŸitim + RAG) kuruluyor...\")\n",
        "!pip install -q -U transformers huggingface_hub accelerate sentence-transformers torch langchain langchain-community langchain-huggingface chromadb pandas peft trl bitsandbytes\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"âœ… Kurulum tamamlandÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DcJb9qLPj0X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADIM 1: VERÄ° HAZIRLIÄI (HafifletilmiÅŸ) ---\n",
        "print(\"ğŸ“Š Veri seti eÄŸitim iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "try:\n",
        "    df_raw = pd.read_csv('netflix_titles.csv')\n",
        "    df_raw = df_raw.fillna('')\n",
        "\n",
        "    # Sadece 100 satÄ±rlÄ±k bir eÄŸitim verisi alÄ±yoruz\n",
        "    train_df = df_raw.head(100)\n",
        "\n",
        "    # Modeli eÄŸiteceÄŸimiz format\n",
        "    def format_row(row):\n",
        "        return f\"Movie: {row['title']}\\nType: {row['type']}\\nDescription: {row['description']}\"\n",
        "\n",
        "    # Pandas'tan HuggingFace Dataset formatÄ±na Ã§evir\n",
        "    dataset = Dataset.from_pandas(train_df)\n",
        "    dataset = dataset.map(lambda x: {\"text\": format_row(x)})\n",
        "\n",
        "    print(\"âœ… Mini eÄŸitim seti hazÄ±rlandÄ±.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Hata: CSV okunurken sorun oldu. {e}\")\n",
        "\n",
        "# --- ADIM 2: MODELÄ° EÄÄ°TÄ°M MODUNDA YÃœKLEME ---\n",
        "print(\"\\nğŸ§  Model 4-bit (EÄŸitim Modu) olarak yÃ¼kleniyor...\")\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# 4-Bit ayarlarÄ±\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# --- ADIM 3: FINE-TUNING (LoRA AyarlarÄ±) ---\n",
        "print(\"\\nğŸ‹ï¸â€â™‚ï¸ Fine-Tuning (EÄŸitim) BaÅŸlÄ±yor... (Bu iÅŸlem 2-3 dk sÃ¼rebilir)\")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sonuclar\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    max_steps=30,\n",
        "    optim=\"paged_adamw_8bit\"\n",
        ")\n",
        "\n",
        "# DÃœZELTME BURADA YAPILDI: dataset_text_field parametresi kaldÄ±rÄ±ldÄ±.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# NOT: EÄŸer yukarÄ±daki kod yine aynÄ± hatayÄ± verirse, 'dataset_text_field=\"text\",' satÄ±rÄ±nÄ± silip tekrar Ã§alÄ±ÅŸtÄ±r.\n",
        "# BazÄ± versiyonlarda gerekmiyor. Ben ÅŸimdilik bÄ±raktÄ±m ama hata devam ederse silmen gerekebilir.\n",
        "# GÃœNCELLEME: Hata almamak iÃ§in garanti yÃ¶ntem o satÄ±rÄ± silmektir. AÅŸaÄŸÄ±da silinmiÅŸ haliyle trainer'Ä± tekrar tanÄ±mlÄ±yorum:\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "# (dataset iÃ§inde zaten \"text\" sÃ¼tunu olduÄŸu iÃ§in otomatik tanÄ±yacaktÄ±r)\n",
        "\n",
        "trainer.train()\n",
        "print(\"âœ… Fine-Tuning tamamlandÄ±! Model artÄ±k Netflix verilerine aÅŸina.\")\n",
        "\n",
        "# --- ADIM 4: EÄÄ°TÄ°LMÄ°Å MODELÄ° RAG Ä°Ã‡Ä°N HAZIRLAMA ---\n",
        "print(\"\\nğŸš€ Chatbot Pipeline oluÅŸturuluyor...\")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=250\n",
        ")\n",
        "print(\"âœ… Sistem kullanÄ±ma hazÄ±r!\")"
      ],
      "metadata": {
        "id": "ARt0EBzsk7Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "print(\"CSV dosyasÄ± okunuyor ve veri formatlanÄ±yor...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('netflix_titles.csv')\n",
        "    df = df.fillna('Bilinmiyor')\n",
        "\n",
        "    dokumanlar = []\n",
        "\n",
        "    # TÃ¼m veriyi alÄ±yoruz\n",
        "    print(f\"Toplam {len(df)} adet kayÄ±t iÅŸleniyor...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # FormatÄ± sadeleÅŸtirdik. BaÅŸlÄ±k en Ã¼stte.\n",
        "        metin = (\n",
        "            f\"BaÅŸlÄ±k: {row['title']}\\n\"\n",
        "            f\"YÃ¶netmen: {row['director']}\\n\"\n",
        "            f\"YÄ±l: {row['release_year']}\\n\"\n",
        "            f\"TÃ¼r: {row['listed_in']}\\n\"\n",
        "            f\"Ã–zet: {row['description']}\\n\"\n",
        "            f\"Detay: Bu film {row['release_year']} yÄ±lÄ±nda yayÄ±nlanmÄ±ÅŸtÄ±r. {row['type']} kategorisindedir.\"\n",
        "        )\n",
        "        dokumanlar.append(metin)\n",
        "\n",
        "    print(f\"âœ… {len(dokumanlar)} adet iÃ§erik baÅŸarÄ±yla hazÄ±rlandÄ±.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ HATA: Dosya yok.\")"
      ],
      "metadata": {
        "id": "BM7eJp19lBO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"VektÃ¶r veritabanÄ± (Ã‡ok Dilli) oluÅŸturuluyor...\")\n",
        "\n",
        "# DEÄÄ°ÅÄ°KLÄ°K BURADA:\n",
        "# ArtÄ±k TÃ¼rkÃ§e-Ä°ngilizce eÅŸleÅŸtirmesi yapabilen 'multilingual' modeli kullanÄ±yoruz.\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# ChromaDB veritabanÄ±nÄ± kur\n",
        "# EÄŸer hata alÄ±rsan Ã¶nceki veritabanÄ±nÄ± temizlemek gerekebilir ama genellikle Ã¼zerine yazar.\n",
        "vektor_veritabani = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "\n",
        "print(\"âœ… Ã‡ok dilli veritabanÄ± hazÄ±r! ArtÄ±k TÃ¼rkÃ§e sorgularÄ± Ä°ngilizce verilerle eÅŸleÅŸtirebilirim.\")"
      ],
      "metadata": {
        "id": "LtbCwX1ylSpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def temizle_yorumlar(metin):\n",
        "    \"\"\"Modelin sonuna eklediÄŸi Ä°ngilizce yorumlarÄ± ve bozuk kÄ±sÄ±mlarÄ± siler.\"\"\"\n",
        "    yasakli_cumleler = [\n",
        "        \"I hope this\", \"Please note\", \"This phrase\", \"The phrase means\",\n",
        "        \"This is a\", \"Translated\", \"Here is\", \"What does\", \"English:\", \"Turkish:\"\n",
        "    ]\n",
        "    satirlar = metin.split('\\n')\n",
        "    temiz_satirlar = []\n",
        "    for satir in satirlar:\n",
        "        if any(yasak in satir for yasak in yasakli_cumleler):\n",
        "            break\n",
        "        if satir.strip():\n",
        "            temiz_satirlar.append(satir)\n",
        "    return \" \".join(temiz_satirlar).strip()\n",
        "\n",
        "def rag_yanit(soru, top_k=3):\n",
        "    # --- HAZIRLIK ---\n",
        "    tur_ceviri = {\n",
        "        \"korku\": \"Horror\", \"gerilim\": \"Thriller\", \"aksiyon\": \"Action\",\n",
        "        \"komedi\": \"Comed\", \"drama\": \"Dram\", \"romantik\": \"Romantic\", \"aÅŸk\": \"Romantic\",\n",
        "        \"bilim kurgu\": \"Sci-Fi\", \"belgesel\": \"Documentary\", \"Ã§ocuk\": \"Children\",\n",
        "        \"aile\": \"Family\", \"anime\": \"Anime\", \"macera\": \"Adventure\", \"suÃ§\": \"Crime\"\n",
        "    }\n",
        "\n",
        "    etkisiz_kelimeler = [\"filmi\", \"film\", \"Ã¶ner\", \"bana\", \"hakkÄ±nda\", \"olan\", \"yapÄ±mÄ±\", \"yÃ¶netmen\", \"oyuncu\", \"kimdir\", \"listesi\", \"var\", \"mÄ±\", \"mu\", \"bir\", \"sÃ¶yle\", \"istiyorum\", \"tavsiye\", \"et\", \"izle\"]\n",
        "\n",
        "    temiz_soru = soru.lower()\n",
        "    for kelime in etkisiz_kelimeler:\n",
        "        temiz_soru = temiz_soru.replace(f\" {kelime} \", \" \").replace(f\" {kelime}\", \"\").replace(f\"{kelime} \", \"\")\n",
        "\n",
        "    anahtar_kelime = temiz_soru.strip()\n",
        "    anahtar_kelimeler_listesi = anahtar_kelime.split()\n",
        "\n",
        "    secilen_film_bilgisi = {}\n",
        "    kaynak_yontem = \"\"\n",
        "    uygun_filmler = pd.DataFrame()\n",
        "\n",
        "    # --- 1. ADIM: ANALÄ°Z ---\n",
        "    yil_tespiti = re.search(r'(\\d{4})', soru)\n",
        "    min_yil = int(df['release_year'].min())\n",
        "    max_yil = int(df['release_year'].max())\n",
        "\n",
        "    # TÃ¼r Tespiti\n",
        "    aranan_tur_eng = None\n",
        "    for tr_tur, eng_tur in tur_ceviri.items():\n",
        "        if tr_tur in soru.lower():\n",
        "            aranan_tur_eng = eng_tur\n",
        "            break\n",
        "\n",
        "    # --- 2. ADIM: ARAMA SENARYOLARI ---\n",
        "\n",
        "    # SENARYO A: YIL VARSA\n",
        "    if yil_tespiti:\n",
        "        aranan_yil = int(yil_tespiti.group(1))\n",
        "        if aranan_yil < min_yil or aranan_yil > max_yil:\n",
        "            return f\"ÃœzgÃ¼nÃ¼m, veri setimde sadece {min_yil} ile {max_yil} yÄ±llarÄ± arasÄ±ndaki filmler bulunmaktadÄ±r. ({aranan_yil} mevcut deÄŸil)\"\n",
        "\n",
        "        kaynak_yontem = f\"YIL FÄ°LTRESÄ° ({aranan_yil})\"\n",
        "        uygun_filmler = df[df['release_year'] == aranan_yil]\n",
        "\n",
        "        # YÄ±lÄ±n iÃ§inde TÃ¼r de varsa sÃ¼z\n",
        "        if aranan_tur_eng and not uygun_filmler.empty:\n",
        "            tur_filtreli = uygun_filmler[uygun_filmler['listed_in'].str.contains(aranan_tur_eng, case=False, na=False)]\n",
        "            if not tur_filtreli.empty:\n",
        "                uygun_filmler = tur_filtreli\n",
        "                kaynak_yontem += f\" + TÃœR ({aranan_tur_eng})\"\n",
        "\n",
        "    # SENARYO B: SADECE TÃœR VARSA (YENÄ° EKLENDÄ°!)\n",
        "    # EÄŸer yÄ±l yoksa ama kullanÄ±cÄ± \"Korku filmi\" dediyse buraya girecek.\n",
        "    elif aranan_tur_eng:\n",
        "        kaynak_yontem = f\"TÃœR FÄ°LTRESÄ° ({aranan_tur_eng})\"\n",
        "        uygun_filmler = df[df['listed_in'].str.contains(aranan_tur_eng, case=False, na=False)]\n",
        "\n",
        "    # SENARYO C: Ä°SÄ°M VARSA (YÄ±l ve TÃ¼r yoksa isimdir)\n",
        "    elif len(anahtar_kelime) > 2:\n",
        "        temp_df_cast = df.copy()\n",
        "        temp_df_dir = df.copy()\n",
        "        for kelime in anahtar_kelimeler_listesi:\n",
        "            temp_df_cast = temp_df_cast[temp_df_cast['cast'].str.contains(kelime, case=False, na=False)]\n",
        "            temp_df_dir = temp_df_dir[temp_df_dir['director'].str.contains(kelime, case=False, na=False)]\n",
        "\n",
        "        uygun_filmler = pd.concat([temp_df_cast, temp_df_dir]).drop_duplicates()\n",
        "        if not uygun_filmler.empty:\n",
        "            kaynak_yontem = f\"Ä°SÄ°M FÄ°LTRESÄ° ('{anahtar_kelime}')\"\n",
        "\n",
        "    # --- 3. ADIM: VERÄ°YÄ° SEÃ‡ME ---\n",
        "    if not uygun_filmler.empty:\n",
        "        secilen = uygun_filmler.sample(n=1).iloc[0]\n",
        "        secilen_film_bilgisi = {\n",
        "            \"title\": secilen['title'],\n",
        "            \"year\": secilen['release_year'],\n",
        "            \"desc\": secilen['description'],\n",
        "            \"genre\": secilen['listed_in']\n",
        "        }\n",
        "        print(f\"\\n[SÄ°STEM: {kaynak_yontem} bulundu: {secilen['title']}]\")\n",
        "\n",
        "    # SENARYO D: VEKTÃ–R ARAMASI (Son Ã‡are)\n",
        "    else:\n",
        "        if yil_tespiti: return f\"AradÄ±ÄŸÄ±nÄ±z kriterlere uygun film bulunamadÄ±.\"\n",
        "\n",
        "        kaynak_yontem = \"VEKTÃ–R ARAMASI\"\n",
        "        docs = vektor_veritabani.similarity_search(soru, k=1)\n",
        "        if docs:\n",
        "            doc = docs[0]\n",
        "            lines = doc.page_content.split('\\n')\n",
        "            title, desc, year, genre = \"Bilinmiyor\", \"AÃ§Ä±klama yok.\", \"???\", \"???\"\n",
        "            for line in lines:\n",
        "                if \"BaÅŸlÄ±k:\" in line: title = line.replace(\"BaÅŸlÄ±k: \", \"\").strip()\n",
        "                if \"Ã–zet:\" in line or \"Description:\" in line: desc = line.split(\":\", 1)[1].strip()\n",
        "                if \"YÄ±l:\" in line: year = line.replace(\"YÄ±l: \", \"\").strip()\n",
        "                if \"TÃ¼r:\" in line: genre = line.replace(\"TÃ¼r: \", \"\").strip()\n",
        "\n",
        "            secilen_film_bilgisi = {\"title\": title, \"year\": year, \"desc\": desc, \"genre\": genre}\n",
        "            print(f\"\\n[SÄ°STEM: VektÃ¶rden bulundu: {title}]\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # --- 4. ADIM: Ã‡EVÄ°RÄ° ---\n",
        "    if not secilen_film_bilgisi: return \"ÃœzgÃ¼nÃ¼m, kriterlerinize uygun bir film bulamadÄ±m.\"\n",
        "\n",
        "    ingilizce_ozet = secilen_film_bilgisi['desc']\n",
        "    prompt = f\"Task: Translate the text below into fluent Turkish. Do not explain. Text: {ingilizce_ozet}\\nTranslation:\"\n",
        "\n",
        "    try:\n",
        "        sonuc = llm_pipeline(prompt, return_full_text=False, max_new_tokens=100)[0][\"generated_text\"]\n",
        "        turkce_ozet = temizle_yorumlar(sonuc.strip())\n",
        "        if len(turkce_ozet) < 5: turkce_ozet = ingilizce_ozet\n",
        "    except:\n",
        "        turkce_ozet = ingilizce_ozet\n",
        "\n",
        "    # --- 5. ADIM: CEVAP ---\n",
        "    final_cevap = (f\"Ã–nerim: {secilen_film_bilgisi['title']} ({secilen_film_bilgisi['year']})\\n\"\n",
        "                   f\"TÃ¼r: {secilen_film_bilgisi['genre']}\\n\"\n",
        "                   f\"Konusu: {turkce_ozet}\")\n",
        "\n",
        "    return final_cevap\n",
        ""
      ],
      "metadata": {
        "id": "kTkrXvTylbfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"ğŸ¬ NETFLIX GURUSU (Ã‡Ä±kmak iÃ§in 'Ã§Ä±kÄ±ÅŸ' yazÄ±n)\")\n",
        "print(\"================================================\\n\")\n",
        "\n",
        "while True:\n",
        "    soru = input(\"Soru sor: \")\n",
        "    if soru.lower() in ['Ã§Ä±kÄ±ÅŸ', 'exit', 'quit']:\n",
        "        print(\"Ä°yi seyirler ğŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    print(\"ğŸ” AraÅŸtÄ±rÄ±yorum...\")\n",
        "    cevap = rag_yanit(soru)\n",
        "    print(f\"\\nğŸ¤– AI: {cevap}\\n\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "MsBFuMchpfXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "kbsvsTNzph2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import warnings\n",
        "\n",
        "# --- SAYFA AYARLARI ---\n",
        "st.set_page_config(page_title=\"Netflix AI Guru\", page_icon=\"ğŸ¬\", layout=\"centered\")\n",
        "st.title(\"ğŸ¬ Netflix AI Bot\")\n",
        "st.markdown(\"*Yapay Zeka Destekli Film & Dizi Ã–neri AsistanÄ±*\")\n",
        "\n",
        "# --- 1. MODEL VE VERÄ° YÃœKLEME (CACHE) ---\n",
        "# Streamlit her tÄ±klamada kodu baÅŸtan Ã§alÄ±ÅŸtÄ±rÄ±r.\n",
        "# Modeli tekrar tekrar yÃ¼klemesin diye @st.cache kullanÄ±yoruz.\n",
        "\n",
        "@st.cache_resource\n",
        "def sistemi_yukle():\n",
        "    print(\"Sistem yÃ¼kleniyor...\")\n",
        "\n",
        "    # 1. Veri Seti\n",
        "    try:\n",
        "        df = pd.read_csv('netflix_titles.csv')\n",
        "        df = df.fillna('')\n",
        "    except:\n",
        "        st.error(\"CSV dosyasÄ± bulunamadÄ±! LÃ¼tfen netflix_titles.csv dosyasÄ±nÄ± yÃ¼kleyin.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # 2. Model (Gemma-2b)\n",
        "    # Not: Fine-tuned modeli kaydetmediysek orijinali kullanÄ±yoruz.\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    llm_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=250\n",
        "    )\n",
        "\n",
        "    # 3. VektÃ¶r VeritabanÄ±\n",
        "    # (HÄ±z kazanmak iÃ§in veri setinin tamamÄ±nÄ± deÄŸil, Ã¶rneÄŸi vektÃ¶rlÃ¼yoruz)\n",
        "    # GerÃ§ek uygulamada bu Ã¶nceden kaydedilip yÃ¼klenir.\n",
        "    dokumanlar = []\n",
        "    # Streamlit RAM'i ÅŸiÅŸmesin diye 3000 satÄ±r alÄ±yoruz\n",
        "    for index, row in df.sample(3000, random_state=42).iterrows():\n",
        "        metin = (\n",
        "            f\"BaÅŸlÄ±k: {row['title']}\\n\"\n",
        "            f\"YÃ¶netmen: {row['director']}\\n\"\n",
        "            f\"YÄ±l: {row['release_year']}\\n\"\n",
        "            f\"TÃ¼r: {row['listed_in']}\\n\"\n",
        "            f\"Ã–zet: {row['description']}\"\n",
        "        )\n",
        "        dokumanlar.append(metin)\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vektor_db = Chroma.from_texts(dokumanlar, embedding=embedding_model)\n",
        "\n",
        "    return df, llm_pipeline, vektor_db, tokenizer\n",
        "\n",
        "# YÃ¼klemeyi BaÅŸlat\n",
        "df, llm_pipeline, vektor_veritabani, tokenizer = sistemi_yukle()\n",
        "\n",
        "# --- 2. RAG FONKSÄ°YONU (BEYÄ°N) ---\n",
        "def rag_yanit(soru):\n",
        "    # Temizlik Fonksiyonu\n",
        "    def temizle_yorumlar(metin):\n",
        "        yasakli = [\"I hope\", \"Please note\", \"English:\", \"Turkish:\", \"Translation:\"]\n",
        "        satirlar = metin.split('\\n')\n",
        "        temiz = []\n",
        "        for s in satirlar:\n",
        "            if any(y in s for y in yasakli): break\n",
        "            if s.strip(): temiz.append(s)\n",
        "        return \" \".join(temiz).strip()\n",
        "\n",
        "    # HazÄ±rlÄ±k\n",
        "    tur_ceviri = {\"korku\": \"Horror\", \"gerilim\": \"Thriller\", \"aksiyon\": \"Action\", \"komedi\": \"Comed\", \"drama\": \"Dram\", \"bilim kurgu\": \"Sci-Fi\", \"aÅŸk\": \"Romantic\"}\n",
        "\n",
        "    # Soru TemizliÄŸi\n",
        "    temiz_soru = soru.lower()\n",
        "    for k in [\"filmi\", \"Ã¶ner\", \"bana\", \"yapÄ±mÄ±\"]:\n",
        "        temiz_soru = temiz_soru.replace(k, \"\")\n",
        "    anahtar = temiz_soru.strip()\n",
        "    keys = anahtar.split()\n",
        "\n",
        "    secilen = {}\n",
        "    yontem = \"\"\n",
        "\n",
        "    # A) YIL KONTROLÃœ\n",
        "    yil_match = re.search(r'(\\d{4})', soru)\n",
        "    uygun = pd.DataFrame()\n",
        "\n",
        "    # TÃ¼r bulma\n",
        "    aranan_tur = None\n",
        "    for tr, eng in tur_ceviri.items():\n",
        "        if tr in soru.lower(): aranan_tur = eng\n",
        "\n",
        "    if yil_match:\n",
        "        yil = int(yil_match.group(1))\n",
        "        uygun = df[df['release_year'] == yil]\n",
        "        yontem = f\"YIL ({yil})\"\n",
        "        if aranan_tur and not uygun.empty:\n",
        "            filt = uygun[uygun['listed_in'].str.contains(aranan_tur, case=False, na=False)]\n",
        "            if not filt.empty: uygun = filt\n",
        "\n",
        "    # B) Ä°SÄ°M KONTROLÃœ\n",
        "    elif len(anahtar) > 2:\n",
        "        temp_c = df.copy()\n",
        "        temp_d = df.copy()\n",
        "        for k in keys:\n",
        "            temp_c = temp_c[temp_c['cast'].str.contains(k, case=False, na=False)]\n",
        "            temp_d = temp_d[temp_d['director'].str.contains(k, case=False, na=False)]\n",
        "        uygun = pd.concat([temp_c, temp_d]).drop_duplicates()\n",
        "        if not uygun.empty: yontem = \"Ä°SÄ°M\"\n",
        "\n",
        "    # SEÃ‡Ä°M\n",
        "    if not uygun.empty:\n",
        "        row = uygun.sample(1).iloc[0]\n",
        "        secilen = {\"title\": row['title'], \"year\": row['release_year'], \"desc\": row['description'], \"genre\": row['listed_in']}\n",
        "    else:\n",
        "        # C) VEKTÃ–R\n",
        "        if yil_match: return \"AradÄ±ÄŸÄ±nÄ±z kriterlere uygun film bulunamadÄ±.\"\n",
        "        docs = vektor_veritabani.similarity_search(soru, k=1)\n",
        "        if docs:\n",
        "            # Basit parse\n",
        "            c = docs[0].page_content\n",
        "            # Burada detaylÄ± parse yerine basitÃ§e contenti alÄ±yoruz\n",
        "            secilen = {\"title\": \"VektÃ¶rden Gelen\", \"year\": \"???\", \"desc\": c, \"genre\": \"Konu BazlÄ±\"}\n",
        "\n",
        "    if not secilen: return \"ÃœzgÃ¼nÃ¼m, sonuÃ§ bulamadÄ±m.\"\n",
        "\n",
        "    # Ã‡EVÄ°RÄ°\n",
        "    if \"BaÅŸlÄ±k:\" in secilen.get('desc', ''): # VektÃ¶rden geldiyse parse et\n",
        "        pass # BasitleÅŸtirildi\n",
        "\n",
        "    ozet = secilen.get('desc', 'No desc')\n",
        "    prompt = f\"Translate to Turkish. No comments.\\nText: {ozet}\\nTranslation:\"\n",
        "\n",
        "    try:\n",
        "        res = llm_pipeline(prompt, return_full_text=False, max_new_tokens=100)[0][\"generated_text\"]\n",
        "        tr_ozet = temizle_yorumlar(res)\n",
        "    except:\n",
        "        tr_ozet = ozet\n",
        "\n",
        "    if secilen['title'] == \"VektÃ¶rden Gelen\":\n",
        "        return f\"Bulunan Ä°Ã§erik:\\n{tr_ozet}\"\n",
        "\n",
        "    return f\"ğŸ¬ **Ã–nerim:** {secilen['title']} ({secilen['year']})\\nğŸ“‚ **TÃ¼r:** {secilen['genre']}\\nğŸ“ **Konusu:** {tr_ozet}\"\n",
        "\n",
        "# --- 3. CHAT ARAYÃœZÃœ ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# GeÃ§miÅŸ mesajlarÄ± ekrana bas\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# KullanÄ±cÄ± giriÅŸi\n",
        "if prompt := st.chat_input(\"Hangi filmi arÄ±yorsun? (Ã–rn: 2019 korku filmi)\"):\n",
        "    # KullanÄ±cÄ± mesajÄ±nÄ± ekle\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # AI CevabÄ±\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Film arÅŸivine bakÄ±lÄ±yor...\"):\n",
        "            response = rag_yanit(prompt)\n",
        "            st.markdown(response)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "vSjdZkPfsFNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "id": "Nll-H5U0sLki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "id": "28k3YFsnsTm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok"
      ],
      "metadata": {
        "id": "g4RPWw_tsXrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata # Colab'in sÄ±r kasasÄ±nÄ± okumak iÃ§in\n",
        "\n",
        "# --- GÃœVENLÄ° BAÅLATMA KODU (COLAB SECRETS) ---\n",
        "\n",
        "try:\n",
        "    # 1. ADIM: Token'Ä± gÃ¼venli kasadan Ã§ek\n",
        "    # userdata.get() fonksiyonu, verdiÄŸin isimdeki sÄ±rrÄ± getirir.\n",
        "    authtoken = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "    # 2. ADIM: ngrok'u yetkilendir ve tÃ¼nel aÃ§\n",
        "    ngrok.set_auth_token(authtoken)\n",
        "    public_url = ngrok.connect(8501)\n",
        "\n",
        "    print(f\"âœ… Web Siten HazÄ±r! Linkin bu: {public_url}\")\n",
        "    print(\"Not: Modelin yÃ¼klenmesi iÃ§in 2-3 dakika beklemen gerekebilir.\")\n",
        "\n",
        "    # 3. ADIM: Streamlit'i arka planda baÅŸlat\n",
        "    !streamlit run app.py &\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ HATA: ngrok token'Ä±nÄ± Colab Secrets'e eklediÄŸinden emin misin?\")\n",
        "    print(f\"Sol menÃ¼deki Anahtar (ğŸ”‘) ikonuna tÄ±kla, 'NGROK_AUTH_TOKEN' adÄ±nda bir sÄ±r oluÅŸtur.\")"
      ],
      "metadata": {
        "id": "cAMSCRaV2BlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zu_zw1LL2LrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}